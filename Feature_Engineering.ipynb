{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**\n",
        "\n",
        "## **Assignment Questions**\n",
        "\n",
        "**Q1.What is a parameter?**\n",
        "  - A **parameter** is a variable that the model learns from the training data. These parameters define how the model makes predictions and decisions. They are adjusted during training to optimize the model's performance.\n"
      ],
      "metadata": {
        "id": "nQSczZUCnNEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.What is correlation?**\n",
        "  **What does negative correlation mean?**\n",
        "  - **Correlation** measures the relationship between two variables and how they move together. In statistics and Machine Learning, it helps determine whether changes in one variable are associated with changes in another.\n",
        " ### **negative correlation:-**\n",
        "  - A negative correlation means that when one variable increases, the other decreases. In other words, they move in opposite directions.\n",
        "**For example:-**\n",
        "- The more time you spend exercising, the less body fat you might have.\n",
        "- The faster a car travels, the less time it takes to reach its destination.\n",
        "\n"
      ],
      "metadata": {
        "id": "S1QL097UoAXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.Define Machine Learning. What are the main components in Machine Learning?**\n",
        "  - ### **Machine Learning (ML) Definition**  \n",
        "Machine Learning is a branch of artificial intelligence (AI) that enables computers to learn patterns from data and make predictions or decisions without being explicitly programmed. It allows systems to improve their performance over time by analyzing and learning from new data.\n",
        "\n",
        " ### **Main Components of Machine Learning**  \n",
        "Machine Learning consists of several essential components:\n",
        "\n",
        "1. **Data** – Raw information used for training the model. Data can be structured (tables, spreadsheets) or unstructured (images, text, videos). Quality data is crucial for accuracy.\n",
        "\n",
        "2. **Features** – Specific attributes or variables used to make predictions. Selecting the right features improves model performance.\n",
        "\n",
        "3. **Model** – The mathematical or statistical algorithm that learns patterns in the data. Examples include decision trees, neural networks, and support vector machines.\n",
        "\n",
        "4. **Training** – The process of feeding data into the model so it can learn. The model adjusts its internal parameters based on patterns it identifies in the data.\n",
        "\n",
        "5. **Loss Function** – A measure of how well the model performs by comparing predicted values to actual values. Lower loss indicates better accuracy.\n",
        "\n",
        "6. **Optimization Algorithm** – Methods like gradient descent that adjust the model parameters to minimize errors and improve performance.\n",
        "\n",
        "7. **Evaluation Metrics** – Techniques such as accuracy, precision, recall, and F1-score used to assess model effectiveness.\n",
        "\n",
        "8. **Deployment** – Applying the trained model to real-world scenarios where it makes predictions or automates tasks."
      ],
      "metadata": {
        "id": "y5WnMpF-pPkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4.How does loss value help in determining whether the model is good or not?**\n",
        "  - A lower loss value generally indicates a better-performing model because it signifies that the model's predictions are closer to the actual target values on the training data. However, it's crucial to also consider the loss on a separate validation set to prevent overfitting. A low training loss but high validation loss suggests the model has merely memorized the training data and won't generalize well."
      ],
      "metadata": {
        "id": "66ny_8XhqBbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.What are continuous and categorical variables?**\n",
        "  - In data science and machine learning, variables are broadly classified into **continuous** and **categorical** variables based on the type of values they represent.\n",
        "\n",
        " ### **Continuous Variables**\n",
        "- These are **numeric** variables that can take an **infinite range of values** within a given interval.\n",
        "- They are typically **measured** rather than counted.\n",
        "- Examples:\n",
        "  - Height (e.g., 170.5 cm)\n",
        "  - Temperature (e.g., 36.7°C)\n",
        "  - Time (e.g., 3.75 seconds)\n",
        "  - Weight (e.g., 62.3 kg)\n",
        "- Continuous variables can have **decimal or fractional values** and are used in regression problems.\n",
        "\n",
        " ### **Categorical Variables**\n",
        "- These are variables that represent **categories** or groups rather than numerical values.\n",
        "- They are typically **counted** rather than measured.\n",
        "- Examples:\n",
        "  - Gender (Male, Female, Non-binary)\n",
        "  - Type of car (SUV, Sedan, Hatchback)\n",
        "  - Color (Red, Blue, Green)\n",
        "  - Education Level (High School, Bachelor's, Master's, PhD)\n",
        "- Categorical variables can be **nominal** (unordered categories) or **ordinal** (ordered categories like \"low\", \"medium\", \"high\")."
      ],
      "metadata": {
        "id": "Nzp0cKfLr3fI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.How do we handle categorical variables in Machine Learning?**\n",
        "   **What are the common techniques?**\n",
        "   - Handling categorical variables in machine learning is essential because most algorithms work with numerical data. Converting categorical variables into a format that models can understand improves accuracy and performance.\n",
        "\n",
        " ### **Common Techniques for Handling Categorical Variables**\n",
        "1. **Label Encoding**  \n",
        "   - Assigns **numeric labels** to each category (e.g., \"Red\" → 0, \"Blue\" → 1, \"Green\" → 2).\n",
        "   - Useful for **ordinal categories** (e.g., \"Low\", \"Medium\", \"High\").\n",
        "   - Works well with **tree-based models** but can cause issues in linear models.\n",
        "\n",
        "2. **One-Hot Encoding**  \n",
        "   - Creates **binary columns** for each category (e.g., \"Red\" → [1, 0, 0], \"Blue\" → [0, 1, 0]).\n",
        "   - Useful for **nominal categorical variables** (no order).\n",
        "   - Can increase dimensionality if there are many unique values.\n",
        "\n",
        "3. **Binary Encoding**  \n",
        "   - Converts categories into **binary numbers** and then into separate columns.\n",
        "   - Reduces dimensionality compared to one-hot encoding.\n",
        "   - Works well for **high-cardinality** categorical variables.\n",
        "\n",
        "4. **Target Encoding**  \n",
        "   - Replaces categories with their **mean target value** (e.g., replacing \"City\" with its average house price).\n",
        "   - Useful for **predictive models**, especially regression.\n",
        "   - Requires careful handling to prevent **data leakage**.\n",
        "\n",
        "5. **Frequency Encoding**  \n",
        "   - Replaces categories with their **frequency of occurrence** (e.g., \"Category A appears 40% of the time\").\n",
        "   - Works well with tree-based models.\n",
        "   - Less prone to increasing dimensionality.\n",
        "\n",
        "6. **Embedding Techniques (for Deep Learning)**  \n",
        "   - Uses neural networks to create **dense vector representations** of categorical variables.\n",
        "   - Reduces dimensionality while maintaining information.\n",
        "   - Especially useful for **natural language processing (NLP)** and complex datasets."
      ],
      "metadata": {
        "id": "5SCdsrhRsike"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.What do you mean by training and testing a dataset?**\n",
        "  - In machine learning, **training** and **testing** a dataset are crucial steps in building a reliable model.\n",
        " ### **Training a Dataset**\n",
        "- This is the phase where the model learns patterns from labeled data.\n",
        "- The dataset is used to adjust the model’s parameters, helping it improve predictions.\n",
        "- Example: If you’re training a model to recognize cats in images, it will analyze cat pictures and learn features like ears, whiskers, and fur patterns.\n",
        "\n",
        " ### **Testing a Dataset**\n",
        "- Once trained, the model is tested on **new, unseen data** to evaluate its accuracy.\n",
        "- Helps determine how well the model generalizes to different situations.\n",
        "- Example: After training the cat-recognition model, you test it on fresh images to see if it correctly identifies cats.\n",
        "\n"
      ],
      "metadata": {
        "id": "XBq1B8WHtQgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.What is sklearn.preprocessing?**\n",
        "  - `sklearn.preprocessing` is a module in **scikit-learn**, a popular Python library for machine learning. This module provides various functions to **prepare and transform** data before feeding it into a machine learning model.\n"
      ],
      "metadata": {
        "id": "GeooalVRuBQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9.What is a Test set?**\n",
        "  - A Test Set is a portion of a dataset used to evaluate the performance of a trained machine learning model. It contains unseen data that the model did not encounter during training, allowing us to measure how well the model generalizes to new inputs.\n"
      ],
      "metadata": {
        "id": "kgpE2feFuZF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10.How do we split data for model fitting (training and testing) in Python?**\n",
        "**How do you approach a Machine Learning problem?**\n",
        "  - **Splitting Data for Model Training & Testing in Python:-**\n",
        "In machine learning, data needs to be divided into training and testing sets to ensure the model learns effectively and generalizes well to unseen data. We typically split the dataset using Scikit-learn's **train_test_split()** function.\n",
        " ### **Approach to Solving a Machine Learning Problem**\n",
        "When tackling a machine learning problem, here’s a structured approach:\n",
        "\n",
        " ### **1. Define the Problem**\n",
        "- Understand the problem statement.\n",
        "- Identify the objective (classification, regression, clustering, etc.).\n",
        "- Determine the key metrics to evaluate success.\n",
        "\n",
        " ### **2. Collect & Prepare the Data**\n",
        "- Gather relevant data from sources (databases, CSV files, APIs).\n",
        "- Handle missing values using **imputation techniques**.\n",
        "- Perform exploratory data analysis (EDA) to understand distributions, correlations, and trends.\n",
        "\n",
        " ### **3. Preprocess & Transform Data**\n",
        "- Standardize or normalize numerical features.\n",
        "- Encode categorical variables (using Label Encoding, One-Hot Encoding, etc.).\n",
        "- Feature engineering to create meaningful variables.\n",
        "\n",
        " ### **4. Choose a Suitable Model**\n",
        "- Select appropriate algorithms (e.g., decision trees, neural networks, support vector machines).\n",
        "- Consider simplicity and interpretability for deployment.\n",
        "\n",
        " ### **5. Train the Model**\n",
        "- Split data into training and test sets.\n",
        "- Use techniques like **cross-validation** to improve robustness.\n",
        "- Optimize hyperparameters using **GridSearchCV** or **RandomizedSearchCV**.\n",
        "\n",
        " ### **6. Evaluate Model Performance**\n",
        "- Use metrics like **accuracy, precision, recall, F1-score, RMSE, or AUC-ROC** to measure effectiveness.\n",
        "- Analyze confusion matrix to assess misclassifications.\n",
        "\n",
        " ### **7. Improve & Fine-Tune**\n",
        "- Experiment with different algorithms or architectures.\n",
        "- Tune hyperparameters for better optimization.\n",
        "- Use techniques like feature selection to reduce overfitting.\n",
        "\n",
        " ### **8. Deploy & Monitor**\n",
        "- Convert the trained model into an API or integrate it into a system.\n",
        "- Continuously monitor model predictions in real-world applications.\n",
        "- Retrain the model when new data is available to maintain performance.\n"
      ],
      "metadata": {
        "id": "a1JuNj3nuoVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11.Why do we have to perform EDA before fitting a model to the data?**\n",
        "  - **Exploratory Data Analysis(**EDA) is a crucial step before fitting a machine learning model because it helps us understand the dataset, detect potential issues, and make informed decisions about preprocessing. Skipping EDA can lead to poor model performance, inaccurate predictions, and misleading insights.\n"
      ],
      "metadata": {
        "id": "ns-e6uZ_v6zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12.What is correlation?**\n",
        "   - **Correlation** measures the relationship between two variables and how they move together. In statistics and Machine Learning, it helps determine whether changes in one variable are associated with changes in another."
      ],
      "metadata": {
        "id": "xqdwov04w_bO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13.What does negative correlation mean?**\n",
        "   - A negative correlation means that when one variable increases, the other decreases. In other words, they move in opposite directions.\n",
        "  **For example:-**\n",
        "    - The more time you spend exercising, the less body fat you might have.\n",
        "    The faster a car travels, the less time it takes to reach its destination.\n"
      ],
      "metadata": {
        "id": "uAI-8P4Mxhnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14.How can you find correlation between variables in Python?**\n",
        "  - We can find the correlation between variables in Python, we use libraries like Pandas, NumPy, or SciPy.\n",
        "  \n"
      ],
      "metadata": {
        "id": "0sAvhqM6x-ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15.What is causation? Explain difference between correlation and causation with an example.**\n",
        "  - Causation refers to a relationship where one event (the cause) directly brings about another event (the effect). In essence, a change in the first event produces a change in the second. It implies a direct mechanism or process linking the two.\n",
        "\n",
        "  - **Difference between correlation and causation:-**\n",
        "\n",
        "  - **Correlation:-** Correlation describes a statistical relationship between two variables. This means that as one variable changes, the other variable tends to change in a predictable way (either in the same direction – positive correlation, or in the opposite direction – negative correlation). However, correlation does not automatically mean that one variable is causing the other to change.\n",
        "\n",
        "  - **Causation:-** Causation goes a step further and indicates that the change in one variable is the direct result of the change in the other variable. There's an underlying mechanism at play.\n"
      ],
      "metadata": {
        "id": "CSPw5I_SHY1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16.What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "  - An optimizer is an algorithm that adjusts the parameters of a machine learning model to minimize errors (loss function) and improve its performance. It determines how the model learns by updating weights during training.\n",
        " ### Types of Optimizers\n",
        "  - some common optimizers used in machine learning:\n",
        "  -  **1.Gradient Descent**\n",
        "  - A basic optimization algorithm that moves the model weights in the direction of the negative gradient to minimize loss.\n",
        "  - **Example:-** In linear regression, gradient descent updates the coefficients to reduce prediction errors.\n"
      ],
      "metadata": {
        "id": "83TiCFTpJTbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simple gradient descent update rule\n",
        "learning_rate = 0.01\n",
        "weight = 2.0\n",
        "gradient = -0.5\n",
        "\n",
        "weight = weight - learning_rate * gradient\n",
        "print(weight)  # Updated weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcC6oE9VKzTf",
        "outputId": "3a8e0900-c6e3-4c62-e0dd-072e6228dc70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - **2.Stochastic Gradient Descent (SGD**)\n",
        "  - Unlike standard gradient descent, SGD updates weights using individual data points rather than the whole dataset.\n",
        "  - **Example:** Used in deep learning models for faster convergence.\n"
      ],
      "metadata": {
        "id": "udGT90toKyZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "zs9xcGql0beQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - **3.Adam (Adaptive Moment Estimation)**\n",
        "  - Combines momentum and adaptive learning rates for efficient optimization.\n",
        "  - **Example:** Commonly used in deep learning models like CNNs and LSTMs.\n"
      ],
      "metadata": {
        "id": "Z_jCW_EoLXYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "snEsjoEcLVCF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **4.RMSprop (Root Mean Square Propagation)**\n",
        " - Adjusts learning rates based on the average of past gradients, preventing rapid oscillations.\n",
        " - Example: Used in recurrent neural networks (RNNs).\n"
      ],
      "metadata": {
        "id": "o0D9iLbdLuXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "kKQe9CUGLtjB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17.What is sklearn.linear_model ?**\n",
        "  - **sklearn.linear_model** is a module within the scikit-learn library, which is one of the most popular and comprehensive machine learning libraries in Python. This specific module is dedicated to linear models."
      ],
      "metadata": {
        "id": "gdUAlaVYL-92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18.What does model.fit() do? What arguments must be given?**\n",
        "   - The `model.fit()` method in Scikit-learn is used to train a machine learning model. It takes in training data, adjusts the model’s parameters, and learns patterns to make predictions.\n",
        "\n",
        " ### **What does `model.fit()` do?**\n",
        "- Takes input data (features) and target values (labels).\n",
        "- Finds the best parameters (e.g., weights in linear models) using an optimization algorithm.\n",
        "- Stores learned parameters for future predictions (`model.predict()`).\n",
        "\n",
        " ### **Required Arguments**\n",
        " - The required arguments depend on the type of model, but generally include:\n",
        " - 1.**`X` (Features/Input Data)** → Independent variables (numerical or categorical).\n",
        " - 2.**`y` (Target/Labels)** → Dependent variable (what the model is predicting).\n",
        "\n",
        " ### **Additional Arguments (Optional)**\n",
        " Some models allow extra parameters:\n",
        " - `sample_weight`: Weights for different samples, useful when some examples are more important.\n",
        " - `epochs & batch_size`: In deep learning models (e.g., TensorFlow/Keras), these define the number of training iterations and sample batches.\n"
      ],
      "metadata": {
        "id": "rqXQhr3qMmaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19.What does model.predict() do? What arguments must be given?**\n",
        "  - The`model.predict()` method in Scikit-learn is used to make predictions based on the trained model. After the model has learned the patterns in the training data, this function applies those learned patterns to new input data.\n",
        "\n",
        " ### **What does `model.predict()` do?**\n",
        " - Takes new feature values (`X`) and applies the learned relationships from training.\n",
        " - Returns predicted values for the given input data.\n",
        "\n",
        " ### **Required Arguments**\n",
        " The method requires:\n",
        " - 1.**`X` (Feature/Input Data)** → The new data points for which predictions are needed.\n",
        "### **Key Points**\n",
        "- The input `X` passed to `model.predict()` must have the same shape (number of features) as the training data.\n",
        "- The method is commonly used for regression (continuous values) and classification (category predictions).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "soU0Nte-Nt5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20.What are continuous and categorical variables?**\n",
        "  - In statistics and data science, variables can generally be classified into **continuous** and **categorical** types based on the nature of their values.\n",
        "\n",
        " ### **1️⃣ Continuous Variables**\n",
        " - Represent numerical values that can take on an infinite range within a given interval.\n",
        " - These variables can be measured and have meaningful mathematical operations like addition, subtraction, and averaging.\n",
        " - **Examples**:\n",
        "    - Height (e.g., 170.5 cm)\n",
        "    - Weight (e.g., 65.2 kg)\n",
        "    - Temperature (e.g., 25.3°C)\n",
        "    - Income (e.g., ₹50,000)\n",
        "\n",
        " ### **2️⃣ Categorical Variables**\n",
        " - Represent discrete categories or labels, often describing characteristics or classifications.\n",
        " - Can be divided into **nominal** (no natural order) or **ordinal** (have a meaningful order).\n",
        " - **Examples**:\n",
        "   - Nominal (No order):\n",
        "     - Gender (Male, Female, Other)\n",
        "     - Eye color (Blue, Brown, Green)\n",
        "   - Ordinal (Has order):\n",
        "    - Education level (High School, Bachelor’s, Master’s)\n",
        "    - Rating (Poor, Average, Good, Excellent)"
      ],
      "metadata": {
        "id": "6gvFKR_HOndn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q21.What is feature scaling? How does it help in Machine Learning?**\n",
        "   - Feature scaling is a technique used in machine learning to normalize or standardize the range of independent variables (features) in a dataset. It ensures that all features contribute equally to the model, preventing biases caused by different scales.\n",
        " ### Feature Scaling Importance:-\n",
        "- Improves Convergence  → Algorithms like gradient descent converge faster when features are on a similar scale.\n",
        "- Enhances Performance → Some models (e.g., SVM, KNN) perform better when features are scaled.\n",
        "- Prevents Dominance  → Large-scale features can overpower smaller-scale ones, leading to biased results.\n"
      ],
      "metadata": {
        "id": "OfsB4W1FPSnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q22.How do we perform scaling in Python?**\n",
        "   - In Python, feature scaling we use the Scikit-learn library, which provides several preprocessing methods to normalize or standardize data.\n",
        "### **Steps to Perform Scaling in Python**\n",
        "   - Import Required Libraries"
      ],
      "metadata": {
        "id": "4oTTMtHuQTFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "tcyBMYTwL8O7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Create Sample Data"
      ],
      "metadata": {
        "id": "arMIK10BTXjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[10], [20], [30], [40], [50]])  # Example feature values"
      ],
      "metadata": {
        "id": "4hZinjiBTh99"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ### Apply Different Scaling Techniques\n",
        "  - Standardization (Z-score normalization)\n",
        "  - Centers data around 0 mean and unit variance.\n"
      ],
      "metadata": {
        "id": "_YUE1aRpTpeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Standardized:\\n\", X_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e9GigoKTl07",
        "outputId": "33f31fbb-7f4b-499b-9499-9eba5adb898f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized:\n",
            " [[-1.41421356]\n",
            " [-0.70710678]\n",
            " [ 0.        ]\n",
            " [ 0.70710678]\n",
            " [ 1.41421356]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  -  Min-Max Scaling (Normalization)\n",
        "  - Rescales values between 0 and 1.\n"
      ],
      "metadata": {
        "id": "xsoNrcAIUAmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Min-Max Scaled:\\n\", X_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3tqlv7OUG_k",
        "outputId": "eca8b9e4-09cb-4136-8f04-89e2026aa89c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min-Max Scaled:\n",
            " [[0.  ]\n",
            " [0.25]\n",
            " [0.5 ]\n",
            " [0.75]\n",
            " [1.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Robust Scaling\n",
        "  - Uses median and interquartile range, making it resistant to outliers.\n"
      ],
      "metadata": {
        "id": "hnUJLn1GUKaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Robust Scaled:\\n\", X_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlsRd6meUUAV",
        "outputId": "5d0f5347-7afa-4cc8-948e-451c9440006f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust Scaled:\n",
            " [[-1. ]\n",
            " [-0.5]\n",
            " [ 0. ]\n",
            " [ 0.5]\n",
            " [ 1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q23.What is sklearn.preprocessing?**\n",
        "   - sklearn.preprocessing is a fundamental module within the scikit-learn (often abbreviated as sklearn) library in Python. It provides a wide array of tools and functions for data preprocessing, which is a crucial step in preparing raw data for machine learning algorithms."
      ],
      "metadata": {
        "id": "MTAhvvQkUlsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q24.How do we split data for model fitting (training and testing) in Python?**\n",
        "   - In Python, we can split data for training and testing using the train_test_split function from Scikit-learn. This ensures the model learns from one portion of the data (training set) and is evaluated on another (testing set) to check its generalization ability.\n"
      ],
      "metadata": {
        "id": "ViQ1Rgi3VE3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q25.Explain data encoding?**\n",
        "   - Data encoding is the process of converting categorical variables into numerical formats so that machine learning models can understand and process them. Since most ML algorithms work with numerical data, encoding is crucial for handling text or categorical features."
      ],
      "metadata": {
        "id": "M_3-kUrTVq9z"
      }
    }
  ]
}